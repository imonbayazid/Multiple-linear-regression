{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own implementation \n",
    "### Multiple linear regression\n",
    "### gradient descent\n",
    "### Stochastic Gradient Descent\n",
    "### Forward and Backward Selection "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Abu Ibne Bayazid (Imon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of multiple linear regression by gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "iteration=30000  \n",
    "\n",
    "def linear_regression_gd (X,y,learning_rate):\n",
    "    featureMatrixSizeWithBias=np.size(X,1) #including bias(one's column)\n",
    "    thetas=np.zeros(featureMatrixSizeWithBias) \n",
    "    totalInstance = len(y)  \n",
    "    costMatrix =[] \n",
    "\n",
    "    for i in range(iteration):       \n",
    "        predictedY=X.dot(thetas) \n",
    "        error=predictedY-y        \n",
    "        thetas = thetas - (learning_rate * (X.T.dot(error) / totalInstance)) # calculate gradient using formula\n",
    "        costAsMSE = mean_square_error(X, y, thetas)\n",
    "        costMatrix.append(costAsMSE)\n",
    "        \n",
    "    return (thetas ,costMatrix) # return as touples \n",
    "\n",
    "def mean_square_error(X,y,theta):\n",
    "    totalInstance = len(y)\n",
    "    MSE = np.sum((X.dot(theta) - y)** 2)/(2* totalInstance)\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extention to implement Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_regression_sgd (X,y,learning_rate, batch_size):\n",
    "    \n",
    "    trainingDataSize=len(y)\n",
    "    featureMatrixSizeWithBias=np.size(X,1)  #including bias(one's column)\n",
    "    thetas=np.zeros(featureMatrixSizeWithBias) \n",
    "    costMatrix =[] \n",
    "    \n",
    "    for i in range(iteration):\n",
    "        shuffledIndexs=np.random.permutation(trainingDataSize) # suffle the training data indexes\n",
    "        XRandomSet=X[shuffledIndexs]\n",
    "        yRandomSet=y[shuffledIndexs]\n",
    "        totalCost=0\n",
    "        \n",
    "        for j in range(0,trainingDataSize,batch_size):\n",
    "            XBatchDataSet= XRandomSet[j:j+batch_size] # get a batch of X\n",
    "            yBatchDataSet=yRandomSet[j:j+batch_size]  # get a batch of y\n",
    "            predictedY=XBatchDataSet.dot(thetas)\n",
    "            error=predictedY-yBatchDataSet\n",
    "            thetas = thetas - (learning_rate * (XBatchDataSet.T.dot(error) / trainingDataSize)) # calculate gradient using formula\n",
    "            cost_MSE = mean_square_error(XBatchDataSet,yBatchDataSet, thetas)\n",
    "            totalCost = totalCost + cost_MSE\n",
    "        costMatrix.append(totalCost)\n",
    "                   \n",
    "    return  (thetas ,costMatrix)     \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loaded  quality of Government Dataset \n",
    "from : https://www.qogdata.pol.gu.se/data/qog_bas_cs_jan18.csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"https://www.qogdata.pol.gu.se/data/qog_bas_cs_jan18.csv\")\n",
    "data=data[[\"cname\",\"wdi_lifexp\",\"wdi_popden\",\"gle_cgdpc\",\"bti_acp\", \"bti_pdi\", \"fh_pair\", \"al_ethnic\",\"al_language\",\"al_religion\",\"bti_aar\",\"vdem_gender\",\"bti_ci\",\"bti_foe\",\"wdi_araland\", \"wdi_forest\"]]\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of all other variables with the life expectancy (wdi_lifexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From custom function \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'wdi_lifexp': 1.0087719298245617,\n",
       " 'wdi_popden': 0.1921300888285124,\n",
       " 'gle_cgdpc': 0.5023396304773133,\n",
       " 'bti_acp': 0.42331241614917875,\n",
       " 'bti_pdi': 0.1993364913893412,\n",
       " 'fh_pair': 0.5142254232268665,\n",
       " 'al_ethnic': -0.606459457714409,\n",
       " 'al_language': -0.6226969196321004,\n",
       " 'al_religion': -0.316991610514409,\n",
       " 'bti_aar': 0.1515196264752576,\n",
       " 'vdem_gender': 0.2027517401678375,\n",
       " 'bti_ci': -0.4270871781268016,\n",
       " 'bti_foe': 0.1546978084446338,\n",
       " 'wdi_araland': 0.024731135841114894,\n",
       " 'wdi_forest': 0.08866177540349948}"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pearsonCorrelationCal(comparingData,comparedWithData):\n",
    "    correlationCoef={}\n",
    "    comparedWith=comparedWithData.values\n",
    "    for columnName in comparingData:\n",
    "        dataA=comparingData[columnName].values\n",
    "        if np.issubdtype(dataA.dtype, np.number) == True:\n",
    "            pearson = np.cov(dataA,comparedWith) [1,0] / (np.std(comparedWith) * np.std(dataA))\n",
    "            correlationCoef[columnName]=pearson\n",
    "    return  correlationCoef\n",
    "\n",
    "\n",
    "comparingData=data.dropna()\n",
    "comparedWithData=comparingData['wdi_lifexp']\n",
    "\n",
    "PCC= pearsonCorrelationCal(comparingData,comparedWithData)\n",
    "print(\"From custom function \")\n",
    "PCC\n",
    "\n",
    "#print(\"From pandas library \")\n",
    "#mdata.corr(method='pearson')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied  GD and SGD\n",
    "comparision with own implementation with scikit learn libraries\n",
    "N:B removed all countries with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With GD \n",
      "Thetas:\n",
      "[2.89604406e-16 3.94906609e-02 6.24556484e-01]\n",
      "Initial cost :  0.495727 \n",
      "Final cost :  0.299000 \n",
      "-----\n",
      "With SGD \n",
      "Thetas:\n",
      "[-3.16027071e-05  3.93549347e-02  6.24474021e-01]\n",
      "Initial cost :  9.416044 \n",
      "Final cost :  5.582674 \n",
      "-----\n",
      "with SGDRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imon_\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thetas:\n",
      "[-1.40456750e-04  3.86951092e-02  6.24129276e-01]\n",
      "-----\n",
      "with LinearRegression\n",
      "Coefficients:  [0.         0.03949066 0.62455648]\n",
      "Intercept:  2.898176307091915e-16\n",
      "Mean squared error: 0.60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dt=data[['wdi_lifexp','wdi_popden','gle_cgdpc']] \n",
    "dt=dt.dropna()\n",
    "\n",
    "X=dt[[\"wdi_popden\",\"gle_cgdpc\"]]\n",
    "y=dt[[\"wdi_lifexp\"]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y).ravel()\n",
    "\n",
    "#need to add ones matrix in the first column of X as x0=1 was assumed\n",
    "ones = np.array([np.ones(len(X))]) \n",
    "ones=np.reshape(ones,(-1,1)) \n",
    "X=np.concatenate((ones,X), axis=1) \n",
    "\n",
    "\n",
    "learning_rate=0.01\n",
    "batch_size=10\n",
    "\n",
    "\n",
    "thetas ,costMatrix = linear_regression_gd(X,y,learning_rate)\n",
    "print('With GD \\nThetas:')\n",
    "print(thetas)\n",
    "print(\"Initial cost :  %f \" % (costMatrix[0]))\n",
    "print(\"Final cost :  %f \" % (costMatrix[-1]))\n",
    "\n",
    "\n",
    "print(\"-----\")\n",
    "thetas ,costMatrix = linear_regression_sgd (X,y,learning_rate, batch_size)\n",
    "print('With SGD \\nThetas:')\n",
    "print(thetas)\n",
    "print(\"Initial cost :  %f \" % (costMatrix[0]))\n",
    "print(\"Final cost :  %f \" % (costMatrix[-1]))\n",
    "\n",
    "print(\"-----\")\n",
    "print('with SGDRegressor')\n",
    "sgd_reg = SGDRegressor(loss='squared_loss',n_iter=iteration, penalty=None, eta0=learning_rate)\n",
    "sgd_reg.fit(X,y)\n",
    "print(\"Thetas:\")\n",
    "print( sgd_reg.coef_)\n",
    "\n",
    "print(\"-----\")\n",
    "print('with LinearRegression')\n",
    "classifier = LinearRegression()\n",
    "model = classifier.fit(X, y)\n",
    "y_predict = np.array(classifier.predict(X))\n",
    "print('Coefficients: ', classifier.coef_)\n",
    "print('Intercept: ', classifier.intercept_)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y, y_predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression models to model the life expectancy (wdi_lifexp) in this dataset\n",
    "Comparision with linear regression, Ridge regression and Lasso using k-fold-cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE value of LinearRegression :  0.947424 \n",
      "MSE value of Ridge Regression :  0.946703 \n",
      "MSE value of Lasso Regression :  0.950462 \n"
     ]
    }
   ],
   "source": [
    "import seaborn as sb\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "data.head()\n",
    "dt=data.drop(['cname'], axis=1)\n",
    "dt=dt.fillna(0)\n",
    "dt=dt.astype(int)\n",
    "\n",
    "\n",
    "features=dt.iloc[:,1:15]\n",
    "target=dt[[\"wdi_lifexp\"]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "target = scaler.fit_transform(target).ravel()\n",
    "\n",
    "#models\n",
    "linerRgsnModel = LinearRegression()\n",
    "ridgeModel = Ridge()\n",
    "lassoModel = Lasso()\n",
    "\n",
    "#apply kfold\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "\n",
    "MSE_LR = model_selection.cross_val_score(linerRgsnModel, features, target, cv=kfold, scoring='neg_mean_squared_error')\n",
    "MSE_LR=np.sqrt(-MSE_LR).mean()\n",
    "print(\"MSE value of LinearRegression :  %f \" % (MSE_LR))\n",
    "\n",
    "MSE_RR = model_selection.cross_val_score(ridgeModel, features, target, cv=kfold, scoring='neg_mean_squared_error')\n",
    "MSE_RR = np.sqrt(-MSE_RR).mean()\n",
    "print(\"MSE value of Ridge Regression :  %f \" % (MSE_RR))\n",
    "\n",
    "MSE_LR = model_selection.cross_val_score(lassoModel, features, target, cv=kfold, scoring='neg_mean_squared_error')\n",
    "MSE_LR=np.sqrt(-MSE_LR).mean()\n",
    "print(\"MSE value of Lasso Regression :  %f \" % (MSE_LR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and Backward Selection algorithms\n",
    "Applied to the Quality of Government dataset.\n",
    "Comparision of the results for Forward and Backward selection with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before applying algorithm, all features  :\n",
      "['wdi_popden', 'gle_cgdpc', 'bti_acp', 'bti_pdi', 'fh_pair', 'al_ethnic', 'al_language', 'al_religion', 'bti_aar', 'vdem_gender', 'bti_ci', 'bti_foe', 'wdi_araland', 'wdi_forest']\n",
      "____________\n",
      "Selected features from forward selection algorithm :\n",
      "['vdem_gender', 'al_language']\n",
      "____________\n",
      "Selected features from backward selection algorithm :\n",
      "['wdi_popden', 'gle_cgdpc', 'al_ethnic', 'al_language', 'vdem_gender', 'wdi_araland']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#using Quality of Government dataset from previous\n",
    "dt=data.drop(['cname'], axis=1)\n",
    "dt=dt.fillna(0)\n",
    "\n",
    "X=dt.iloc[:,1:15]\n",
    "y=dt[[\"wdi_lifexp\"]].values\n",
    "\n",
    "\n",
    "\n",
    "def forwardSelectionAlgo(X,y): \n",
    "    allFeatures=X.columns.tolist()\n",
    "    remainingFeatures=allFeatures\n",
    "    includedFeatures=[] # start with empty list\n",
    "    for i in range(len(allFeatures)):\n",
    "        pVal = []\n",
    "        for fcolumn in remainingFeatures:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[includedFeatures+[fcolumn]]))).fit() \n",
    "            pVal.append(model.pvalues[fcolumn])    \n",
    "        minPval = min(pVal) # get the minimum p value    \n",
    "        if minPval < .01:   # check if the p value is less than .01 then we will take the feature to our model    \n",
    "            includedFeatures.append(remainingFeatures[pVal.index(minPval)]) # include the feature to our model\n",
    "            del remainingFeatures[pVal.index(minPval)]\n",
    "    return includedFeatures    \n",
    "\n",
    "def backwardSelectionAlgo(X,y): \n",
    "    allFeatures=X.columns.tolist()\n",
    "    remainingFeatures=allFeatures  # start with all features in the list\n",
    "    for i in range(len(allFeatures)):\n",
    "        pVal = []\n",
    "        for fcolumn in remainingFeatures:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[fcolumn]))).fit()\n",
    "            pVal.append(model.pvalues[fcolumn])    \n",
    "        maxPval = max(pVal)  # get the maximum p value   \n",
    "        if maxPval > .05:  # check if the p value is greater than .05 then we will remove the feature from our model       \n",
    "            #del selectedFeatures[remainingFeatures[pVal.index(maxPval)]] # remove the feature from our model \n",
    "            del remainingFeatures[pVal.index(maxPval)] # remove the feature from our model\n",
    "    return remainingFeatures\n",
    "\n",
    "print('Before applying algorithm, all features  :')\n",
    "print(X.columns.tolist()) \n",
    "\n",
    "print(\"____________\")\n",
    "selectedFeaturesByFS = forwardSelectionAlgo(X, y)\n",
    "print('Selected features from forward selection algorithm :')\n",
    "print(selectedFeaturesByFS)   \n",
    "\n",
    "print(\"____________\")\n",
    "selectedFeaturesByBS = backwardSelectionAlgo(X, y)\n",
    "print('Selected features from backward selection algorithm :')\n",
    "print(selectedFeaturesByBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
